{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataracing Challenge 2023\n",
    "\n",
    "## Predicting loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import lightning\n",
    "import optuna\n",
    "import sklearn\n",
    "import os\n",
    "from sklearn.metrics import log_loss\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from lightning.pytorch.trainer.trainer import Trainer\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.prediction_writer import BasePredictionWriter\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from optuna.trial import Trial\n",
    "from optuna.study import Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.Random(\"BL6ADS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRACT_ID,BORROWER_ID,CONTRACT_BANK_ID,CONTRACT_CREDIT_INTERMEDIARY,CONTRACT_CREDIT_LOSS,CONTRACT_CURRENCY,CONTRACT_DATE_OF_LOAN_AGREEMENT,CONTRACT_DEPT_SERVICE_TO_INCOME,CONTRACT_FREQUENCY_TYPE,CONTRACT_INCOME,CONTRACT_INSTALMENT_AMOUNT,CONTRACT_INSTALMENT_AMOUNT_2,CONTRACT_INTEREST_PERIOD,CONTRACT_INTEREST_RATE,CONTRACT_LGD,CONTRACT_LOAN_AMOUNT,CONTRACT_LOAN_CONTRACT_TYPE,CONTRACT_LOAN_TO_VALUE_RATIO,CONTRACT_LOAN_TYPE,CONTRACT_MARKET_VALUE,CONTRACT_MATURITY_DATE,CONTRACT_MORTGAGE_LENDING_VALUE,CONTRACT_MORTGAGE_TYPE,CONTRACT_REFINANCED,CONTRACT_RISK_WEIGHTED_ASSETS,CONTRACT_TYPE_OF_INTEREST_REPAYMENT,BORROWER_BIRTH_YEAR,BORROWER_CITIZENSHIP,BORROWER_COUNTRY,BORROWER_COUNTY,BORROWER_TYPE_OF_CUSTOMER,BORROWER_TYPE_OF_SETTLEMENT,TARGET_EVENT,TARGET_EVENT_DAY\n",
    "# TpK8osXs,d8SqtuEV,1d42bbf5,2.0,0.0,31,2457052,,479a2e13,,9545.0,11059.0,,22.08,175424.0,192161,1,,69f70539,,2457798,,,1.0,1.0,,1217.0,98.0,98.0,20.0,A,,-,\n",
    "# EtIEHrcH,lrdxML0g,1d42bbf5,,0.0,31,2457036,,479a2e13,,9528.0,,,16.57,994868.0,55590,2,,b503a0de,,2457217,784680.0,5.0,,74.17,,,,,,A,,-,\n",
    "# 1G10DfKj,gII7nnq4,1d42bbf5,2.0,16350.0,31,2457043,7.05,479a2e13,127305.0,8899.0,8880.0,50.0,22.35,282553.0,275924,2,,b503a0de,,2458530,,,2.0,74.77,100003.0,1199.0,98.0,98.0,179.0,A,7.0,-,\n",
    "# 2NLT774,MMkJ8z/e,1d42bbf5,,0.0,31,2457038,,479a2e13,,14329.0,12966.0,,0.0,68981.0,138862,2,,b503a0de,,2457345,,,,0.99,,1221.0,98.0,98.0,,A,,-,\n",
    "# VpylRvay,M417onFP,1d42bbf5,2.0,2395.0,31,2457091,,479a2e13,,4003.0,3901.0,37.0,23.92,163319.0,169624,1,,69f70539,,2459648,,,2.0,74.3,100002.0,1260.0,98.0,98.0,178.0,A,1.0,-,\n",
    "# 5tsDwTjS,u9GRAuen,1d42bbf5,2.0,0.0,31,2457093,,479a2e13,,11214.0,10727.0,,35.12,74244.0,100079,2,,b503a0de,,2457393,,,2.0,0.98,,1204.0,98.0,98.0,,A,,-,\n",
    "# drZAWXaR,XPWFcJZI,1d42bbf5,,0.0,31,2457095,33.94,479a2e13,172647.0,10933.0,,,21.93,337632.0,338415,1,,69f70539,,2458556,,,,1.0,,,,,,A,,-,\n",
    "# ebtK6u+,L2Nu2zZe,1d42bbf5,2.0,0.0,31,2457046,,479a2e13,,11405.0,11423.0,,16.28,118161.0,140975,2,,b503a0de,,2457562,,,2.0,1.03,,1255.0,98.0,98.0,4.0,A,,-,\n",
    "# ffJh4MY,5A0GWOi2,1d42bbf5,2.0,0.0,31,2457049,,479a2e13,,6045.0,6008.0,,16.27,42047.0,60963,2,,b503a0de,,2457441,,,2.0,1.04,,1234.0,98.0,98.0,,A,,-,\n",
    "# eo09vYuo,lqPGd2+x,1d42bbf5,2.0,0.0,31,2457081,,479a2e13,,10431.0,10420.0,,21.66,170739.0,190782,1,,69f70539,,2457814,,,2.0,1.0,0.0,1228.0,98.0,98.0,3.0,A,,-,\n",
    "df = pd.read_csv('data/training_data.csv', sep=',', header=0, index_col=False)\n",
    "\n",
    "# Print few rows\n",
    "print(df.head())\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "borrower_ids = {value: idx for idx, value in enumerate(df['BORROWER_ID'].unique())}\n",
    "borrower_ids_from_enum = {idx: value for idx, value in enumerate(df['BORROWER_ID'].unique())}\n",
    "\n",
    "# CONTRACT_ID - Enum\n",
    "def contract_id_enum_transform(df: DataFrame) -> DataFrame:\n",
    "    contract_ids = {value: idx for idx, value in enumerate(df['CONTRACT_ID'].unique())}\n",
    "    df['CONTRACT_ID'] = df['CONTRACT_ID'].map(contract_ids)\n",
    "    return df\n",
    "\n",
    "# BORROWER_ID - Enum\n",
    "def borrower_id_enum_transform(df: DataFrame) -> DataFrame:\n",
    "    df['BORROWER_ID'] = df['BORROWER_ID'].map(borrower_ids)\n",
    "    return df\n",
    "\n",
    "# CONTRACT_BANK_ID - Enum\n",
    "def contract_bank_id_enum_transform(df: DataFrame) -> DataFrame:\n",
    "    contract_bank_ids = {value: idx for idx, value in enumerate(df['CONTRACT_BANK_ID'].unique())}\n",
    "    df['CONTRACT_BANK_ID'] = df['CONTRACT_BANK_ID'].map(contract_bank_ids)\n",
    "    return df\n",
    "\n",
    "# CONTRACT_CREDIT_INTERMEDIARY - Enum\n",
    "def contract_credit_intermediary_enum_transform(df: DataFrame) -> DataFrame:\n",
    "    contract_credit_intermediary_ids = {value: idx for idx, value in enumerate(df['CONTRACT_CREDIT_INTERMEDIARY'].unique())}\n",
    "    df['CONTRACT_CREDIT_INTERMEDIARY'] = df['CONTRACT_CREDIT_INTERMEDIARY'].map(contract_credit_intermediary_ids)\n",
    "    return df\n",
    "\n",
    "# CONTRACT_CURRENCY - Enum\n",
    "def contract_currency_enum_transform(df: DataFrame) -> DataFrame:\n",
    "    contract_currency_ids = {value: idx for idx, value in enumerate(df['CONTRACT_CURRENCY'].unique())}\n",
    "    df['CONTRACT_CURRENCY'] = df['CONTRACT_CURRENCY'].map(contract_currency_ids)\n",
    "    return df\n",
    "\n",
    "# CONTRACT_FREQUENCY_TYPE - Enum\n",
    "def contract_frequency_type_enum_transform(df: DataFrame) -> DataFrame:\n",
    "    contract_frequency_type_ids = {value: idx for idx, value in enumerate(df['CONTRACT_FREQUENCY_TYPE'].unique())}\n",
    "    df['CONTRACT_FREQUENCY_TYPE'] = df['CONTRACT_FREQUENCY_TYPE'].map(contract_frequency_type_ids)\n",
    "    return df\n",
    "\n",
    "# CONTRACT_INCOME - Nan to 0\n",
    "def contract_income_nan_to_zero_transform(df: DataFrame) -> DataFrame:\n",
    "    df['CONTRACT_INCOME'] = df['CONTRACT_INCOME'].fillna(0)\n",
    "    return df\n",
    "\n",
    "# BORROWER_CITIZENSHIP - Float to Int\n",
    "def borrower_citizenship_int_transform(df: DataFrame) -> DataFrame:\n",
    "    df['BORROWER_CITIZENSHIP'] = df['BORROWER_CITIZENSHIP'].fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "# BORROWER_COUNTRY - Float to Int\n",
    "def borrower_country_int_transform(df: DataFrame) -> DataFrame:\n",
    "    df['BORROWER_COUNTRY'] = df['BORROWER_COUNTRY'].fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "# BORROWER_COUNTY - Float to Int\n",
    "def borrower_county_int_transform(df: DataFrame) -> DataFrame:\n",
    "    df['BORROWER_COUNTY'] = df['BORROWER_COUNTY'].fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "# BORROWER_TYPE_OF_CUSTOMER - Enum\n",
    "def borrower_type_of_customer_enum_transform(df: DataFrame) -> DataFrame:\n",
    "    borrower_type_of_customer_ids = {value: idx for idx, value in enumerate(df['BORROWER_TYPE_OF_CUSTOMER'].unique())}\n",
    "    df['BORROWER_TYPE_OF_CUSTOMER'] = df['BORROWER_TYPE_OF_CUSTOMER'].map(borrower_type_of_customer_ids)\n",
    "    return df\n",
    "\n",
    "# BORROWER_TYPE_OF_SETTLEMENT - Enum\n",
    "def borrower_type_of_settlement_enum_transform(df: DataFrame) -> DataFrame:\n",
    "    borrower_type_of_settlement_ids = {value: idx for idx, value in enumerate(df['BORROWER_TYPE_OF_SETTLEMENT'].unique())}\n",
    "    df['BORROWER_TYPE_OF_SETTLEMENT'] = df['BORROWER_TYPE_OF_SETTLEMENT'].map(borrower_type_of_settlement_ids)\n",
    "    return df\n",
    "\n",
    "# TARGET_EVENT - Bool (1 if 'K', 0 otherwise)\n",
    "def target_event_bool_transform(df: DataFrame) -> DataFrame:\n",
    "    df['TARGET_EVENT'] = df['TARGET_EVENT'].map({'K': True, '': False, 'E': False}).astype(bool)\n",
    "    return df\n",
    "\n",
    "def df_to_32_bit_numeric_transform(df: DataFrame) -> DataFrame:\n",
    "    df = df.fillna(0.0)\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == pd.StringDtype:\n",
    "            # Enum transform\n",
    "            column_values = {value: idx for idx, value in enumerate(df[column].unique())}\n",
    "            df[column] = df[column].map(column_values).astype(pd.Int32Dtype())\n",
    "        if df[column].dtype == pd.Float64Dtype:\n",
    "            df[column] = df[column].astype(pd.Float32Dtype())\n",
    "        if df[column].dtype == pd.Int64Dtype:\n",
    "            df[column] = df[column].astype(pd.Int32Dtype())\n",
    "    return df\n",
    "\n",
    "def apply_transformations(tfs: list, df: DataFrame) -> DataFrame:\n",
    "    for tf in tfs:\n",
    "        df = tf(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = df.copy()\n",
    "tfs = [\n",
    "    # contract_id_enum_transform,\n",
    "    # borrower_id_enum_transform,\n",
    "    contract_bank_id_enum_transform,\n",
    "    contract_credit_intermediary_enum_transform,\n",
    "    contract_currency_enum_transform,\n",
    "    contract_frequency_type_enum_transform,\n",
    "    contract_income_nan_to_zero_transform,\n",
    "    borrower_citizenship_int_transform,\n",
    "    borrower_country_int_transform,\n",
    "    borrower_county_int_transform,\n",
    "    borrower_type_of_customer_enum_transform,\n",
    "    borrower_type_of_settlement_enum_transform,\n",
    "    target_event_bool_transform,\n",
    "    df_to_32_bit_numeric_transform\n",
    "]\n",
    "\n",
    "df = apply_transformations(tfs, original_df.copy())\n",
    "\n",
    "def get_train_test_split(df: DataFrame, train_size=40000, test_size=10000) -> list[DataFrame]:\n",
    "    return train_test_split(\n",
    "        df.drop(columns=['CONTRACT_ID', 'BORROWER_ID', 'TARGET_EVENT', 'TARGET_EVENT_DAY']), # Drop target column from training data\n",
    "        df['TARGET_EVENT'], # Target column\n",
    "        random_state=42,\n",
    "        train_size=40000,\n",
    "        test_size=10000)\n",
    "\n",
    "train_x, test_x, train_y, test_y = get_train_test_split(df)\n",
    "train_x_float = train_x.astype(float)\n",
    "test_x_float = test_x.astype(float)\n",
    "train_y_float = train_y.astype(float)\n",
    "test_y_float = test_y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS\n",
    "\n",
    "\n",
    "class PropertyDataModule(lightning.LightningDataModule):\n",
    "    def __init__(self, train_x, test_x, train_y, test_y, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_x = train_x\n",
    "        self.test_x = test_x\n",
    "        self.train_y = train_y\n",
    "        self.test_y = test_y\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        self.train_x_tensor = torch.tensor(self.train_x.values, dtype=torch.float32)\n",
    "        self.test_x_tensor = torch.tensor(self.test_x.values, dtype=torch.float32)\n",
    "        self.train_y_tensor = torch.tensor(self.train_y.values, dtype=torch.float32)\n",
    "        self.test_y_tensor = torch.tensor(self.test_y.values, dtype=torch.float32)\n",
    "        self.train_dataset = TensorDataset(self.train_x_tensor, self.train_y_tensor)\n",
    "        self.test_dataset = TensorDataset(self.test_x_tensor, self.test_y_tensor)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, pin_memory=True) # num_workers=os.cpu_count()\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, pin_memory=True) # num_workers=os.cpu_count()\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, pin_memory=True) # num_workers=os.cpu_count()\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, pin_memory=True) # num_workers=os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class DeepNN(lightning.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        hidden_size: int,\n",
    "        dropout: float,\n",
    "        learning_rate: float):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.input_size, self.hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(self.dropout),\n",
    "            torch.nn.Linear(self.hidden_size, self.output_size),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.net(x)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = torch.nn.functional.binary_cross_entropy(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.net(x)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = torch.nn.functional.binary_cross_entropy(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.net(x)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = torch.nn.functional.binary_cross_entropy(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        y_hat = self.net(x)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = PropertyDataModule(batch_size=64, train_x=train_x_float, test_x=test_x_float, train_y=train_y_float, test_y=test_y_float)\n",
    "\n",
    "def trial_run(trial: Trial) -> float:\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0.001,\n",
    "        patience=8,\n",
    "        verbose=True,\n",
    "        mode='min'\n",
    "    )\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath='models/',\n",
    "        filename='loan_ml_prediction-{epoch}-{val_loss:.2f}',\n",
    "        save_top_k=3,\n",
    "        mode='min',\n",
    "\n",
    "    )\n",
    "    logger = pl_loggers.TensorBoardLogger('logs/', name='loan_ml_prediction')\n",
    "    net = DeepNN(\n",
    "        input_size=train_x.shape[1],\n",
    "        output_size=1,\n",
    "        hidden_size=trial.suggest_int('hidden_size', 2, 40, log=True),\n",
    "        dropout=trial.suggest_float('dropout', 0.0, 0.5),\n",
    "        learning_rate=trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        max_epochs=40,\n",
    "        accelerator='gpu',\n",
    "        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "        logger=logger,\n",
    "    )\n",
    "    trainer.fit(net, data_module)\n",
    "    trainer.test(net, datamodule=data_module)\n",
    "    loss = trainer.callback_metrics['test_loss'].item()\n",
    "    trial.set_user_attr('test_loss', loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=30, interval_steps=10),\n",
    "    sampler=optuna.samplers.TPESampler(multivariate=True, seed=42)\n",
    "    )\n",
    "study.optimize(trial_run, n_trials=10)\n",
    "print(f'Best trial: {study.best_trial.value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(list_of_lists, flat_list=None):\n",
    "    if not flat_list:\n",
    "        flat_list = []\n",
    "    if not list_of_lists:\n",
    "        return flat_list\n",
    "    else:\n",
    "        for item in list_of_lists:\n",
    "            if type(item) == list:\n",
    "                flatten_list(item, flat_list)\n",
    "            else:\n",
    "                flat_list.append(item)\n",
    "\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BORROWER_ID and keep only the rows in df which contains the BORROWER_ID from df_submission\n",
    "df_submission = pd.read_csv('data/data_submission_example.csv', sep=',', header=0, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique TARGET_EVENT values and their count\n",
    "df = original_df.copy()\n",
    "print(f'Target event unique values', df['TARGET_EVENT'].unique())\n",
    "print(f'Target event unique values count', df['TARGET_EVENT'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = original_df.copy()\n",
    "print(f'Shared BORROWER_ID elements between df_submission and df', len(set(df_submission['BORROWER_ID'].unique()).intersection(df['BORROWER_ID'].unique())))\n",
    "# Keep only the rows in df which contains the BORROWER_ID from df_submission\n",
    "df = df[df['BORROWER_ID'].isin(df_submission['BORROWER_ID'].unique())]\n",
    "print(f'Shared BORROWER_ID elements between df_submission and df after filtering', len(df))\n",
    "print(f'Number of unique BORROWER_ID in df', len(df['BORROWER_ID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = original_df.copy()\n",
    "# Keep only the rows in df which contains the BORROWER_ID from df_submission\n",
    "df = df[df['BORROWER_ID'].isin(df_submission['BORROWER_ID'].unique())]\n",
    "\n",
    "# df = df[df['BORROWER_ID'].isin(df_submission['BORROWER_ID'])]\n",
    "# print(df.head())\n",
    "# number of rows in df where TARGET_EVENT is NaN\n",
    "# df = df[df['TARGET_EVENT'] != '-']\n",
    "# print(f'DF without target event length', len(df[df['TARGET_EVENT'] != '-']))\n",
    "print(f'DF length', len(df))\n",
    "# number of rows in df_submission\n",
    "print(f'DF Submission length', len(df_submission))\n",
    "\n",
    "borrower_ids_from_enum = {idx: value for idx, value in enumerate(df['BORROWER_ID'].unique())}\n",
    "df = apply_transformations(tfs, df)\n",
    "\n",
    "val_x = df.drop(columns=['CONTRACT_ID', 'BORROWER_ID', 'TARGET_EVENT', 'TARGET_EVENT_DAY']) # Drop target column from training data\n",
    "val_y = df['TARGET_EVENT'] # Target column\n",
    "\n",
    "val_x_float = val_x.astype(float)\n",
    "val_y_float = val_y.astype(float)\n",
    "\n",
    "val_data_module = PropertyDataModule(batch_size=32, train_x=val_x_float, test_x=val_x_float, train_y=val_y, test_y=val_y)\n",
    "\n",
    "checkpoint_path = None\n",
    "# Load lowest loss checkpoint (dirname = models, filename = loan_ml_prediction-{epoch}-{val_loss:.2f})\n",
    "for file in os.listdir('models/'):\n",
    "    if file.startswith('loan_ml_prediction') and file.endswith('.ckpt'):\n",
    "        # Check if loss is lower than current checkpoint\n",
    "        if checkpoint_path is None:\n",
    "            checkpoint_path = f'models/{file}'\n",
    "            continue\n",
    "        # loan_ml_prediction-epoch=0-val_loss=2.59-v5.ckpt\n",
    "        # loan_ml_prediction-epoch=0-val_loss=2.59.ckpt\n",
    "        # Get the 2.59 as float\n",
    "        file_loss = float(file.removesuffix('.ckpt').split('=')[2].split('-')[0])\n",
    "        current_loss = float(checkpoint_path.removesuffix('.ckpt').split('=')[2].split('-')[0])\n",
    "        if float(file_loss < current_loss):\n",
    "            checkpoint_path = f'models/{file}'\n",
    "\n",
    "if checkpoint_path is None:\n",
    "    raise Exception('No checkpoint found')\n",
    "net = DeepNN.load_from_checkpoint(checkpoint_path)\n",
    "\n",
    "df_submission = pd.read_csv('data/data_submission_example.csv', sep=',', header=0, index_col=False)\n",
    "\n",
    "class CustomWriter(BasePredictionWriter):\n",
    "    def __init__(self, output_dir, write_interval):\n",
    "        super().__init__(write_interval)\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def write_on_epoch_end(self, trainer, pl_module, predictions, batch_indices):\n",
    "        global df_submission\n",
    "        global df\n",
    "        # print(f'Raw Predictions', predictions)\n",
    "        # Predictions is a list of tensors, make it a single list\n",
    "        # Last is a 0 dim tensor, make it a list first\n",
    "        predictions = [p.tolist() for p in predictions]\n",
    "        predictions = [element for sublist in predictions for element in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "        # New column in df: PRED with predictions\n",
    "        df['PRED'] = predictions\n",
    "        # Group by BORROWER_ID and get the mean of PRED\n",
    "        # Need to subtract PRED from 1\n",
    "        unique_preds = 1 - df.groupby('BORROWER_ID')['PRED'].mean()\n",
    "        unique_borrowers = df['BORROWER_ID'].map(borrower_ids_from_enum).unique()\n",
    "        df = pd.DataFrame({'BORROWER_ID': unique_borrowers, 'PRED': unique_preds})\n",
    "        df.describe()\n",
    "        # Keep rows which appear in BORROWER_ID from df_submission\n",
    "        # df = df[df['BORROWER_ID'].isin(df_submission['BORROWER_ID'].unique())]\n",
    "        df.to_csv('data/predictions.csv', index=False)\n",
    "        # Keep only BORROWER_ID and PRED columns\n",
    "        # print(f'Submissions DataFrame size', len(df_submission))\n",
    "\n",
    "pred_writer = CustomWriter(output_dir=\"pred_path\", write_interval=\"epoch\")\n",
    "trainer = Trainer(accelerator='gpu', callbacks=[pred_writer])\n",
    "predictions = trainer.predict(net, datamodule=val_data_module, return_predictions=True)\n",
    "# predictions = trainer.pred\n",
    "# print(predictions)\n",
    "\n",
    "# if len(predicted_values) != len(df_submission):\n",
    "#     raise ValueError(\"Length of predictions does not match length of submission DataFrame.\")\n",
    "\n",
    "# df_submission['PRED'] = predicted_values\n",
    "# df_submission.to_csv('data/predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
