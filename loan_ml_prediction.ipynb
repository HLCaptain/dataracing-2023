{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataracing Challenge 2023\n",
    "\n",
    "## Predicting loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\illya\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\illya\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 14.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import lightning\n",
    "import optuna\n",
    "import sklearn\n",
    "import os\n",
    "import math\n",
    "import lightgbm as lgb\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from lightning.pytorch.trainer.trainer import Trainer\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.prediction_writer import BasePredictionWriter\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from optuna.trial import Trial\n",
    "from optuna.study import Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.Random(\"BL6ADS\")\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CONTRACT_ID BORROWER_ID CONTRACT_BANK_ID  CONTRACT_CREDIT_INTERMEDIARY  \\\n",
      "0    TpK8osXs    d8SqtuEV         1d42bbf5                           2.0   \n",
      "1    EtIEHrcH    lrdxML0g         1d42bbf5                           NaN   \n",
      "2    1G10DfKj    gII7nnq4         1d42bbf5                           2.0   \n",
      "3     2NLT774    MMkJ8z/e         1d42bbf5                           NaN   \n",
      "4    VpylRvay    M417onFP         1d42bbf5                           2.0   \n",
      "\n",
      "   CONTRACT_CREDIT_LOSS  CONTRACT_CURRENCY  CONTRACT_DATE_OF_LOAN_AGREEMENT  \\\n",
      "0                   0.0                 31                          2457052   \n",
      "1                   0.0                 31                          2457036   \n",
      "2               16350.0                 31                          2457043   \n",
      "3                   0.0                 31                          2457038   \n",
      "4                2395.0                 31                          2457091   \n",
      "\n",
      "   CONTRACT_DEPT_SERVICE_TO_INCOME CONTRACT_FREQUENCY_TYPE  CONTRACT_INCOME  \\\n",
      "0                              NaN                479a2e13              NaN   \n",
      "1                              NaN                479a2e13              NaN   \n",
      "2                             7.05                479a2e13         127305.0   \n",
      "3                              NaN                479a2e13              NaN   \n",
      "4                              NaN                479a2e13              NaN   \n",
      "\n",
      "   ...  CONTRACT_RISK_WEIGHTED_ASSETS  CONTRACT_TYPE_OF_INTEREST_REPAYMENT  \\\n",
      "0  ...                           1.00                                  NaN   \n",
      "1  ...                          74.17                                  NaN   \n",
      "2  ...                          74.77                             100003.0   \n",
      "3  ...                           0.99                                  NaN   \n",
      "4  ...                          74.30                             100002.0   \n",
      "\n",
      "   BORROWER_BIRTH_YEAR  BORROWER_CITIZENSHIP  BORROWER_COUNTRY  \\\n",
      "0               1217.0                  98.0              98.0   \n",
      "1                  NaN                   NaN               NaN   \n",
      "2               1199.0                  98.0              98.0   \n",
      "3               1221.0                  98.0              98.0   \n",
      "4               1260.0                  98.0              98.0   \n",
      "\n",
      "   BORROWER_COUNTY  BORROWER_TYPE_OF_CUSTOMER  BORROWER_TYPE_OF_SETTLEMENT  \\\n",
      "0             20.0                          A                          NaN   \n",
      "1              NaN                          A                          NaN   \n",
      "2            179.0                          A                          7.0   \n",
      "3              NaN                          A                          NaN   \n",
      "4            178.0                          A                          1.0   \n",
      "\n",
      "  TARGET_EVENT  TARGET_EVENT_DAY  \n",
      "0            -               NaN  \n",
      "1            -               NaN  \n",
      "2            -               NaN  \n",
      "3            -               NaN  \n",
      "4            -               NaN  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# CONTRACT_ID,BORROWER_ID,CONTRACT_BANK_ID,CONTRACT_CREDIT_INTERMEDIARY,CONTRACT_CREDIT_LOSS,CONTRACT_CURRENCY,CONTRACT_DATE_OF_LOAN_AGREEMENT,CONTRACT_DEPT_SERVICE_TO_INCOME,CONTRACT_FREQUENCY_TYPE,CONTRACT_INCOME,CONTRACT_INSTALMENT_AMOUNT,CONTRACT_INSTALMENT_AMOUNT_2,CONTRACT_INTEREST_PERIOD,CONTRACT_INTEREST_RATE,CONTRACT_LGD,CONTRACT_LOAN_AMOUNT,CONTRACT_LOAN_CONTRACT_TYPE,CONTRACT_LOAN_TO_VALUE_RATIO,CONTRACT_LOAN_TYPE,CONTRACT_MARKET_VALUE,CONTRACT_MATURITY_DATE,CONTRACT_MORTGAGE_LENDING_VALUE,CONTRACT_MORTGAGE_TYPE,CONTRACT_REFINANCED,CONTRACT_RISK_WEIGHTED_ASSETS,CONTRACT_TYPE_OF_INTEREST_REPAYMENT,BORROWER_BIRTH_YEAR,BORROWER_CITIZENSHIP,BORROWER_COUNTRY,BORROWER_COUNTY,BORROWER_TYPE_OF_CUSTOMER,BORROWER_TYPE_OF_SETTLEMENT,TARGET_EVENT,TARGET_EVENT_DAY\n",
    "# TpK8osXs,d8SqtuEV,1d42bbf5,2.0,0.0,31,2457052,,479a2e13,,9545.0,11059.0,,22.08,175424.0,192161,1,,69f70539,,2457798,,,1.0,1.0,,1217.0,98.0,98.0,20.0,A,,-,\n",
    "# EtIEHrcH,lrdxML0g,1d42bbf5,,0.0,31,2457036,,479a2e13,,9528.0,,,16.57,994868.0,55590,2,,b503a0de,,2457217,784680.0,5.0,,74.17,,,,,,A,,-,\n",
    "# 1G10DfKj,gII7nnq4,1d42bbf5,2.0,16350.0,31,2457043,7.05,479a2e13,127305.0,8899.0,8880.0,50.0,22.35,282553.0,275924,2,,b503a0de,,2458530,,,2.0,74.77,100003.0,1199.0,98.0,98.0,179.0,A,7.0,-,\n",
    "# 2NLT774,MMkJ8z/e,1d42bbf5,,0.0,31,2457038,,479a2e13,,14329.0,12966.0,,0.0,68981.0,138862,2,,b503a0de,,2457345,,,,0.99,,1221.0,98.0,98.0,,A,,-,\n",
    "# VpylRvay,M417onFP,1d42bbf5,2.0,2395.0,31,2457091,,479a2e13,,4003.0,3901.0,37.0,23.92,163319.0,169624,1,,69f70539,,2459648,,,2.0,74.3,100002.0,1260.0,98.0,98.0,178.0,A,1.0,-,\n",
    "# 5tsDwTjS,u9GRAuen,1d42bbf5,2.0,0.0,31,2457093,,479a2e13,,11214.0,10727.0,,35.12,74244.0,100079,2,,b503a0de,,2457393,,,2.0,0.98,,1204.0,98.0,98.0,,A,,-,\n",
    "# drZAWXaR,XPWFcJZI,1d42bbf5,,0.0,31,2457095,33.94,479a2e13,172647.0,10933.0,,,21.93,337632.0,338415,1,,69f70539,,2458556,,,,1.0,,,,,,A,,-,\n",
    "# ebtK6u+,L2Nu2zZe,1d42bbf5,2.0,0.0,31,2457046,,479a2e13,,11405.0,11423.0,,16.28,118161.0,140975,2,,b503a0de,,2457562,,,2.0,1.03,,1255.0,98.0,98.0,4.0,A,,-,\n",
    "# ffJh4MY,5A0GWOi2,1d42bbf5,2.0,0.0,31,2457049,,479a2e13,,6045.0,6008.0,,16.27,42047.0,60963,2,,b503a0de,,2457441,,,2.0,1.04,,1234.0,98.0,98.0,,A,,-,\n",
    "# eo09vYuo,lqPGd2+x,1d42bbf5,2.0,0.0,31,2457081,,479a2e13,,10431.0,10420.0,,21.66,170739.0,190782,1,,69f70539,,2457814,,,2.0,1.0,0.0,1228.0,98.0,98.0,3.0,A,,-,\n",
    "df = pd.read_csv('data/training_data.csv', sep=',', header=0, index_col=False)\n",
    "\n",
    "# Print few rows\n",
    "print(df.head())\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "borrower_ids = {value: idx for idx, value in enumerate(df['BORROWER_ID'].unique())}\n",
    "borrower_ids_from_enum = {idx: value for idx, value in enumerate(df['BORROWER_ID'].unique())}\n",
    "\n",
    "# CONTRACT_ID - Enum\n",
    "def contract_id_enum_transform(df: DataFrame) -> DataFrame:\n",
    "    contract_ids = {value: idx for idx, value in enumerate(df['CONTRACT_ID'].unique())}\n",
    "    df['CONTRACT_ID'] = df['CONTRACT_ID'].map(contract_ids)\n",
    "    return df\n",
    "\n",
    "# BORROWER_ID - Enum\n",
    "def borrower_id_enum_transform(df: DataFrame) -> DataFrame:\n",
    "    df['BORROWER_ID'] = df['BORROWER_ID'].map(borrower_ids)\n",
    "    return df\n",
    "\n",
    "# CONTRACT_BANK_ID - Enum\n",
    "def contract_bank_id_enum_transform(df: DataFrame) -> DataFrame:\n",
    "    contract_bank_ids = {value: idx for idx, value in enumerate(df['CONTRACT_BANK_ID'].unique())}\n",
    "    df['CONTRACT_BANK_ID'] = df['CONTRACT_BANK_ID'].map(contract_bank_ids)\n",
    "    return df\n",
    "\n",
    "# CONTRACT_CREDIT_INTERMEDIARY - Enum\n",
    "def contract_credit_intermediary_enum_transform(df: DataFrame) -> DataFrame:\n",
    "    contract_credit_intermediary_ids = {value: idx for idx, value in enumerate(df['CONTRACT_CREDIT_INTERMEDIARY'].unique())}\n",
    "    df['CONTRACT_CREDIT_INTERMEDIARY'] = df['CONTRACT_CREDIT_INTERMEDIARY'].map(contract_credit_intermediary_ids)\n",
    "    return df\n",
    "\n",
    "# CONTRACT_CURRENCY - Enum\n",
    "def contract_currency_enum_transform(df: DataFrame) -> DataFrame:\n",
    "    contract_currency_ids = {value: idx for idx, value in enumerate(df['CONTRACT_CURRENCY'].unique())}\n",
    "    df['CONTRACT_CURRENCY'] = df['CONTRACT_CURRENCY'].map(contract_currency_ids)\n",
    "    return df\n",
    "\n",
    "# CONTRACT_FREQUENCY_TYPE - Enum\n",
    "def contract_frequency_type_enum_transform(df: DataFrame) -> DataFrame:\n",
    "    contract_frequency_type_ids = {value: idx for idx, value in enumerate(df['CONTRACT_FREQUENCY_TYPE'].unique())}\n",
    "    df['CONTRACT_FREQUENCY_TYPE'] = df['CONTRACT_FREQUENCY_TYPE'].map(contract_frequency_type_ids)\n",
    "    return df\n",
    "\n",
    "# CONTRACT_INCOME - Nan to 0\n",
    "def contract_income_nan_to_zero_transform(df: DataFrame) -> DataFrame:\n",
    "    df['CONTRACT_INCOME'] = df['CONTRACT_INCOME'].fillna(0)\n",
    "    return df\n",
    "\n",
    "# BORROWER_CITIZENSHIP - Float to Int\n",
    "def borrower_citizenship_int_transform(df: DataFrame) -> DataFrame:\n",
    "    df['BORROWER_CITIZENSHIP'] = df['BORROWER_CITIZENSHIP'].fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "# BORROWER_COUNTRY - Float to Int\n",
    "def borrower_country_int_transform(df: DataFrame) -> DataFrame:\n",
    "    df['BORROWER_COUNTRY'] = df['BORROWER_COUNTRY'].fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "# BORROWER_COUNTY - Float to Int\n",
    "def borrower_county_int_transform(df: DataFrame) -> DataFrame:\n",
    "    df['BORROWER_COUNTY'] = df['BORROWER_COUNTY'].fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "# BORROWER_TYPE_OF_CUSTOMER - Enum\n",
    "def borrower_type_of_customer_enum_transform(df: DataFrame) -> DataFrame:\n",
    "    borrower_type_of_customer_ids = {value: idx for idx, value in enumerate(df['BORROWER_TYPE_OF_CUSTOMER'].unique())}\n",
    "    df['BORROWER_TYPE_OF_CUSTOMER'] = df['BORROWER_TYPE_OF_CUSTOMER'].map(borrower_type_of_customer_ids)\n",
    "    return df\n",
    "\n",
    "# BORROWER_TYPE_OF_SETTLEMENT - Enum\n",
    "def borrower_type_of_settlement_enum_transform(df: DataFrame) -> DataFrame:\n",
    "    borrower_type_of_settlement_ids = {value: idx for idx, value in enumerate(df['BORROWER_TYPE_OF_SETTLEMENT'].unique())}\n",
    "    df['BORROWER_TYPE_OF_SETTLEMENT'] = df['BORROWER_TYPE_OF_SETTLEMENT'].map(borrower_type_of_settlement_ids)\n",
    "    return df\n",
    "\n",
    "# TARGET_EVENT - Bool (1 if 'K', 0 otherwise)\n",
    "def target_event_bool_transform(df: DataFrame) -> DataFrame:\n",
    "    df['TARGET_EVENT'] = df['TARGET_EVENT'].map({'K': False, '': True, 'E': True}).astype(bool)\n",
    "    return df\n",
    "\n",
    "def df_to_32_bit_numeric_transform(df: DataFrame) -> DataFrame:\n",
    "    df = df.fillna(0.0)\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == pd.StringDtype:\n",
    "            # Enum transform\n",
    "            column_values = {value: idx for idx, value in enumerate(df[column].unique())}\n",
    "            df[column] = df[column].map(column_values).astype(pd.Int32Dtype())\n",
    "        if df[column].dtype == pd.Float64Dtype:\n",
    "            df[column] = df[column].astype(pd.Float32Dtype())\n",
    "        if df[column].dtype == pd.Int64Dtype:\n",
    "            df[column] = df[column].astype(pd.Int32Dtype())\n",
    "    return df\n",
    "\n",
    "def apply_transformations(tfs: list, df: DataFrame) -> DataFrame:\n",
    "    for tf in tfs:\n",
    "        df = tf(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = [\n",
    "    # contract_id_enum_transform,\n",
    "    # borrower_id_enum_transform,\n",
    "    contract_bank_id_enum_transform,\n",
    "    contract_credit_intermediary_enum_transform,\n",
    "    contract_currency_enum_transform,\n",
    "    contract_frequency_type_enum_transform,\n",
    "    contract_income_nan_to_zero_transform,\n",
    "    borrower_citizenship_int_transform,\n",
    "    borrower_country_int_transform,\n",
    "    borrower_county_int_transform,\n",
    "    borrower_type_of_customer_enum_transform,\n",
    "    borrower_type_of_settlement_enum_transform,\n",
    "    target_event_bool_transform,\n",
    "    df_to_32_bit_numeric_transform\n",
    "]\n",
    "\n",
    "keep_columns = [\n",
    "    'CONTRACT_BANK_ID',\n",
    "    'CONTRACT_CREDIT_INTERMEDIARY',\n",
    "    'CONTRACT_CURRENCY',\n",
    "    'CONTRACT_FREQUENCY_TYPE',\n",
    "    'CONTRACT_INCOME',\n",
    "    'BORROWER_CITIZENSHIP',\n",
    "    'BORROWER_COUNTRY',\n",
    "    'BORROWER_COUNTY',\n",
    "    'BORROWER_TYPE_OF_CUSTOMER',\n",
    "    'BORROWER_TYPE_OF_SETTLEMENT',\n",
    "    'TARGET_EVENT',\n",
    "]\n",
    "\n",
    "original_df = df.copy()\n",
    "\n",
    "df = apply_transformations(tfs, original_df.copy())[keep_columns]\n",
    "\n",
    "def get_train_test_split(df: DataFrame, train_size, test_size) -> list[DataFrame]:\n",
    "    return train_test_split(\n",
    "        df.drop(columns=['TARGET_EVENT']), # Drop target column from training data\n",
    "        df['TARGET_EVENT'], # Target column\n",
    "        random_state=42,\n",
    "        train_size=train_size,\n",
    "        test_size=test_size)\n",
    "\n",
    "train_x, test_x, train_y, test_y = get_train_test_split(df, 160000, 40000)\n",
    "train_x_float = train_x.astype(float)\n",
    "test_x_float = test_x.astype(float)\n",
    "train_y_float = train_y.astype(float)\n",
    "test_y_float = test_y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS\n",
    "\n",
    "class PropertyDataModule(lightning.LightningDataModule):\n",
    "    def __init__(self, train_x, test_x, train_y, test_y, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_x = train_x\n",
    "        self.test_x = test_x\n",
    "        self.train_y = train_y\n",
    "        self.test_y = test_y\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        self.train_x_tensor = torch.tensor(self.train_x.values, dtype=torch.float32)\n",
    "        self.test_x_tensor = torch.tensor(self.test_x.values, dtype=torch.float32)\n",
    "        self.train_y_tensor = torch.tensor(self.train_y.values, dtype=torch.float32)\n",
    "        self.test_y_tensor = torch.tensor(self.test_y.values, dtype=torch.float32)\n",
    "        self.train_dataset = TensorDataset(self.train_x_tensor, self.train_y_tensor)\n",
    "        self.test_dataset = TensorDataset(self.test_x_tensor, self.test_y_tensor)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, pin_memory=True, drop_last=True, persistent_workers=True, num_workers=os.cpu_count())\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, pin_memory=True, drop_last=True, persistent_workers=True, num_workers=os.cpu_count())\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=len(self.test_dataset.tensors), shuffle=False, pin_memory=True, drop_last=True, persistent_workers=True,num_workers=os.cpu_count())\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=len(self.test_dataset.tensors), shuffle=False, pin_memory=True, drop_last=True, persistent_workers=True, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class DeepNN(lightning.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        learning_rate: float):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.input_size, self.output_size),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = torch.nn.functional.binary_cross_entropy(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = torch.nn.functional.binary_cross_entropy(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = torch.nn.functional.binary_cross_entropy(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        # self.log('test_rocauc', roc_auc_score(y.cpu(), y_hat.cpu()))\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_run(trial: Trial) -> float:\n",
    "    data_module = PropertyDataModule(batch_size=2**trial.suggest_int('batch_size', 4, 10), train_x=train_x_float, test_x=test_x_float, train_y=train_y_float, test_y=test_y_float)\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0.001,\n",
    "        patience=10,\n",
    "        verbose=True,\n",
    "        mode='min'\n",
    "    )\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath='models/',\n",
    "        filename='loan_ml_prediction-{epoch}-{val_loss:.2f}',\n",
    "        save_top_k=1,\n",
    "        mode='min',\n",
    "    )\n",
    "    logger = pl_loggers.TensorBoardLogger('logs/', name='loan_ml_prediction')\n",
    "    net = DeepNN(\n",
    "        input_size=train_x.shape[1],\n",
    "        output_size=1,\n",
    "        learning_rate=trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        max_epochs=100,\n",
    "        accelerator='gpu',\n",
    "        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "        logger=logger,\n",
    "    )\n",
    "    trainer.fit(net, data_module)\n",
    "    trainer.test(net, datamodule=data_module)\n",
    "    loss = trainer.callback_metrics['test_loss'].item()\n",
    "    # rocauc = trainer.callback_metrics['test_rocauc'].item()\n",
    "    trial.set_user_attr('test_loss', loss)\n",
    "    # trial.set_user_attr('test_rocauc', rocauc)\n",
    "    # return loss / math.exp(rocauc)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def trial_run_lgb(trial: Trial) -> float:\n",
    "    # Define the hyperparameters to be tuned\n",
    "    # Run on gpu\n",
    "    param = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'device': 'gpu',\n",
    "    }\n",
    "\n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(train_x_float, label=train_y_float)\n",
    "    valid_data = lgb.Dataset(test_x_float, label=test_y_float, reference=train_data)\n",
    "\n",
    "    # Callback to save the best model\n",
    "    def save_best_model_cb(env):\n",
    "        if env.iteration == env.end_iteration - 1:\n",
    "            # Save the model at the last iteration\n",
    "            best_loss = env.evaluation_result_list[0][2]  # Extract the loss\n",
    "            model_filename = f\"models/loan_ml_prediction-trial=0-val_loss={best_loss:.2f}.txt\"\n",
    "            env.model.save_model(model_filename)\n",
    "            print(f\"Best model saved to {model_filename}\")\n",
    "\n",
    "    # Train the model\n",
    "    model = lgb.train(param, train_data, valid_sets=[valid_data], callbacks=[save_best_model_cb])\n",
    "\n",
    "    # Make predictions and calculate loss\n",
    "    preds = model.predict(test_x_float, num_iteration=model.best_iteration)\n",
    "    loss = mean_squared_error(test_y_float, preds)\n",
    "\n",
    "    # Save the metrics\n",
    "    trial.set_user_attr('test_loss', loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_float32_matmul_precision('high')\n",
    "# study = optuna.create_study(\n",
    "#     direction='minimize',\n",
    "#     pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10, interval_steps=5),\n",
    "#     sampler=optuna.samplers.TPESampler(multivariate=True, group=True, seed=42)\n",
    "#     )\n",
    "# study.optimize(trial_run_lgb, n_trials=200)\n",
    "# print(f'Best trial: {study.best_trial.value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(list_of_lists, flat_list=None):\n",
    "    if not flat_list:\n",
    "        flat_list = []\n",
    "    if not list_of_lists:\n",
    "        return flat_list\n",
    "    else:\n",
    "        for item in list_of_lists:\n",
    "            if type(item) == list:\n",
    "                flatten_list(item, flat_list)\n",
    "            else:\n",
    "                flat_list.append(item)\n",
    "\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BORROWER_ID and keep only the rows in df which contains the BORROWER_ID from df_submission\n",
    "df_submission = pd.read_csv('data/data_submission_example.csv', sep=',', header=0, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target event unique values ['-' 'K' 'E']\n",
      "Target event unique values count TARGET_EVENT\n",
      "-    1548364\n",
      "E      43515\n",
      "K      10874\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Number of unique TARGET_EVENT values and their count\n",
    "df = original_df.copy()\n",
    "print(f'Target event unique values', df['TARGET_EVENT'].unique())\n",
    "print(f'Target event unique values count', df['TARGET_EVENT'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared BORROWER_ID elements between df_submission and df 1117674\n",
      "Shared BORROWER_ID elements between df_submission and df after filtering 1564601\n",
      "Number of unique BORROWER_ID in df 1117674\n"
     ]
    }
   ],
   "source": [
    "df = original_df.copy()\n",
    "print(f'Shared BORROWER_ID elements between df_submission and df', len(set(df_submission['BORROWER_ID'].unique()).intersection(df['BORROWER_ID'].unique())))\n",
    "# Keep only the rows in df which contains the BORROWER_ID from df_submission\n",
    "df = df[df['BORROWER_ID'].isin(df_submission['BORROWER_ID'].unique())]\n",
    "print(f'Shared BORROWER_ID elements between df_submission and df after filtering', len(df))\n",
    "print(f'Number of unique BORROWER_ID in df', len(df['BORROWER_ID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF length 1564601\n",
      "DF Submission length 1117674\n"
     ]
    }
   ],
   "source": [
    "df = original_df.copy()\n",
    "# Keep only the rows in df which contains the BORROWER_ID from df_submission\n",
    "df = df[df['BORROWER_ID'].isin(df_submission['BORROWER_ID'].unique())]\n",
    "\n",
    "# df = df[df['BORROWER_ID'].isin(df_submission['BORROWER_ID'])]\n",
    "# print(df.head())\n",
    "# number of rows in df where TARGET_EVENT is NaN\n",
    "# df = df[df['TARGET_EVENT'] != '-']\n",
    "# print(f'DF without target event length', len(df[df['TARGET_EVENT'] != '-']))\n",
    "print(f'DF length', len(df))\n",
    "# number of rows in df_submission\n",
    "print(f'DF Submission length', len(df_submission))\n",
    "\n",
    "borrower_ids_from_enum = {idx: value for idx, value in enumerate(df['BORROWER_ID'].unique())}\n",
    "df = apply_transformations(tfs, df)\n",
    "\n",
    "# Select columns for validation\n",
    "val_x = df[keep_columns].drop(columns=['TARGET_EVENT'])  # Drop target column\n",
    "val_y = df['TARGET_EVENT']  # Target column\n",
    "\n",
    "# Convert to float if necessary\n",
    "val_x_float = val_x.astype(float)\n",
    "val_y_float = val_y.astype(float)\n",
    "\n",
    "# Load the trained LightGBM model\n",
    "model_path = None\n",
    "\n",
    "for file in os.listdir('models/'):\n",
    "    if file.startswith('loan_ml_prediction') and file.endswith('.txt'):\n",
    "        # Check if loss is lower than current checkpoint\n",
    "        if model_path is None:\n",
    "            model_path = f'models/{file}'\n",
    "            continue\n",
    "        # loan_ml_prediction-epoch=0-val_loss=2.59-v5.txt\n",
    "        # loan_ml_prediction-epoch=0-val_loss=2.59.txt\n",
    "        # Get the 2.59 as float\n",
    "        file_loss = float(file.removesuffix('.txt').split('=')[2].split('-')[0])\n",
    "        current_loss = float(model_path.removesuffix('.txt').split('=')[2].split('-')[0])\n",
    "        if float(file_loss < current_loss):\n",
    "            checkpoint_path = f'models/{file}'\n",
    "\n",
    "if model_path is None:\n",
    "    raise Exception('No checkpoint found')\n",
    "# model_path = 'path_to_your_lightgbm_model.txt'  # Replace with your model's filename\n",
    "bst = lgb.Booster(model_file=model_path)\n",
    "\n",
    "# Make predictions\n",
    "predictions = bst.predict(val_x_float)\n",
    "\n",
    "# Map predictions back to borrower IDs\n",
    "df['PRED'] = predictions\n",
    "unique_preds = 1 - df.groupby('BORROWER_ID')['PRED'].mean()\n",
    "unique_borrowers = df['BORROWER_ID'].map(borrower_ids_from_enum).unique()\n",
    "df_submission = pd.DataFrame({'BORROWER_ID': unique_borrowers, 'PRED': unique_preds})\n",
    "\n",
    "# Optionally, normalize or adjust the predictions as needed\n",
    "# For example, if you need to normalize to a specific average value:\n",
    "# target_average = 0.0148\n",
    "# df_submission['PRED'] = df_submission['PRED'] / df_submission['PRED'].mean() * target_average\n",
    "\n",
    "# Save the predictions\n",
    "df_submission.to_csv('data/predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF length 1564601\n",
      "DF Submission length 1117674\n",
      "Loaded checkpoint models/loan_ml_prediction-epoch=31-val_loss=0.61.ckpt with hparams \"input_size\":    10\n",
      "\"learning_rate\": 0.0026070247583707684\n",
      "\"output_size\":   1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "df = original_df.copy()\n",
    "# Keep only the rows in df which contains the BORROWER_ID from df_submission\n",
    "df = df[df['BORROWER_ID'].isin(df_submission['BORROWER_ID'].unique())]\n",
    "\n",
    "# df = df[df['BORROWER_ID'].isin(df_submission['BORROWER_ID'])]\n",
    "# print(df.head())\n",
    "# number of rows in df where TARGET_EVENT is NaN\n",
    "# df = df[df['TARGET_EVENT'] != '-']\n",
    "# print(f'DF without target event length', len(df[df['TARGET_EVENT'] != '-']))\n",
    "print(f'DF length', len(df))\n",
    "# number of rows in df_submission\n",
    "print(f'DF Submission length', len(df_submission))\n",
    "\n",
    "borrower_ids_from_enum = {idx: value for idx, value in enumerate(df['BORROWER_ID'].unique())}\n",
    "df = apply_transformations(tfs, df)\n",
    "\n",
    "val_x = df[keep_columns].drop(columns=['TARGET_EVENT']) # Drop target column from training data\n",
    "val_y = df['TARGET_EVENT'] # Target column\n",
    "\n",
    "val_x_float = val_x.astype(float)\n",
    "val_y_float = val_y.astype(float)\n",
    "\n",
    "val_data_module = PropertyDataModule(batch_size=32, train_x=val_x_float, test_x=val_x_float, train_y=val_y, test_y=val_y)\n",
    "\n",
    "checkpoint_path = None\n",
    "# Load lowest loss checkpoint (dirname = models, filename = loan_ml_prediction-{epoch}-{val_loss:.2f})\n",
    "for file in os.listdir('models/'):\n",
    "    if file.startswith('loan_ml_prediction') and file.endswith('.ckpt'):\n",
    "        # Check if loss is lower than current checkpoint\n",
    "        if checkpoint_path is None:\n",
    "            checkpoint_path = f'models/{file}'\n",
    "            continue\n",
    "        # loan_ml_prediction-epoch=0-val_loss=2.59-v5.ckpt\n",
    "        # loan_ml_prediction-epoch=0-val_loss=2.59.ckpt\n",
    "        # Get the 2.59 as float\n",
    "        file_loss = float(file.removesuffix('.ckpt').split('=')[2].split('-')[0])\n",
    "        current_loss = float(checkpoint_path.removesuffix('.ckpt').split('=')[2].split('-')[0])\n",
    "        if float(file_loss < current_loss):\n",
    "            checkpoint_path = f'models/{file}'\n",
    "\n",
    "if checkpoint_path is None:\n",
    "    raise Exception('No checkpoint found')\n",
    "# checkpoint_path = 'models/loan_ml_prediction-epoch=26-val_loss=0.05.ckpt'\n",
    "net = DeepNN.load_from_checkpoint(checkpoint_path)\n",
    "net.eval()\n",
    "\n",
    "print(f'Loaded checkpoint {checkpoint_path} with hparams {net.hparams}')\n",
    "\n",
    "df_submission = pd.read_csv('data/data_submission_example.csv', sep=',', header=0, index_col=False)\n",
    "\n",
    "class CustomWriter(BasePredictionWriter):\n",
    "    def __init__(self, output_dir, write_interval):\n",
    "        super().__init__(write_interval)\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def write_on_epoch_end(self, trainer, pl_module, predictions, batch_indices):\n",
    "        global df_submission\n",
    "        global df\n",
    "        # print(f'Raw Predictions', predictions)\n",
    "        # Predictions is a list of tensors, make it a single list\n",
    "        # Last is a 0 dim tensor, make it a list first\n",
    "        predictions = [p.tolist() for p in predictions]\n",
    "        predictions = [element for sublist in predictions for element in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "        # New column in df: PRED with predictions\n",
    "        df['PRED'] = predictions\n",
    "        # Group by BORROWER_ID and get the mean of PRED\n",
    "        # Need to subtract PRED from 1\n",
    "        unique_preds = 1 - df.groupby('BORROWER_ID')['PRED'].mean()\n",
    "        unique_borrowers = df['BORROWER_ID'].map(borrower_ids_from_enum).unique()\n",
    "        df = pd.DataFrame({'BORROWER_ID': unique_borrowers, 'PRED': unique_preds})\n",
    "        df.describe()\n",
    "        # Keep rows which appear in BORROWER_ID from df_submission\n",
    "        # df = df[df['BORROWER_ID'].isin(df_submission['BORROWER_ID'].unique())]\n",
    "        # Normalize PRED column values to avarage 0.0148\n",
    "        # df['PRED'] = df['PRED'] / df['PRED'].mean() * 0.0148 # This is wrong\n",
    "        df.to_csv('data/predictions.csv', index=False)\n",
    "        # Keep only BORROWER_ID and PRED columns\n",
    "        # print(f'Submissions DataFrame size', len(df_submission))\n",
    "\n",
    "pred_writer = CustomWriter(output_dir=\"pred_path\", write_interval=\"epoch\")\n",
    "trainer = Trainer(accelerator='gpu', callbacks=[pred_writer])\n",
    "# predictions = trainer.predict(net, datamodule=val_data_module, return_predictions=True)\n",
    "# predictions = trainer.pred\n",
    "# print(predictions)\n",
    "\n",
    "# if len(predicted_values) != len(df_submission):\n",
    "#     raise ValueError(\"Length of predictions does not match length of submission DataFrame.\")\n",
    "\n",
    "# df_submission['PRED'] = predicted_values\n",
    "# df_submission.to_csv('data/predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
